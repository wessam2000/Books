{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPyDByNY1NAhUv1s60Ei0nH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wessam2000/Books/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B_j7NxjzJiE-"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals \n",
        "\n",
        "import tensorflow as tf\n",
        "#  !pip install -q imageio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') \n",
        "img_path = 'gdrive/mydrive/pic/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR-1o9lpVbJs",
        "outputId": "6e6d02bb-d5d6-49fe-f290-f8a763715cf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import amax\n",
        "from PIL import Image\n",
        "dataset= []\n",
        "DATA_SIZE = 88\n",
        "AMPILIFCATION = 10\n",
        "for i in range(1,DATA_SIZE):\n",
        "  for j in range(AMPILIFCATION):\n",
        "     image = Image.open(img_path,mode='r')\n",
        "     dataset.append((np.asarray(image)-127.5)/127.5)"
      ],
      "metadata": {
        "id": "IhplZFkQeCWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "BUFFER_SIZE = 60000\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "NsWdai8tgTDT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATOR\n",
        "def make_generator_model():\n",
        "   model = tf.keras.Sequential()\n",
        "   model.add(layers.Dense(32*32*1024, use_bias=False, input_shape=(100,)))\n",
        "   model.add(layers.BatchNormalization()) \n",
        "   model.add(layers.LeakyReLU())\n",
        "   model.add(layers.Reshape((32, 32, 1024))) \n",
        "   assert model.output_shape == (None, 32, 32, 1024) \n",
        "\n",
        "   model.add(layers.Conv2DTranspose(512, (3,3), strides=(2,2),padding='same' ,use_bias=False))\n",
        "   assert model.output_shape == (None, 64, 64, 512)\n",
        "   model.add(layers.BatchNormalization()) \n",
        "   model.add(layers.LeakyReLU())\n",
        "\n",
        "   model.add(layers.Conv2DTranspose(256, (3,3), strides=(2,2),padding='same' ,use_bias=False))\n",
        "   assert model.output_shape == (None, 128, 128, 256) \n",
        "   model.add(layers.BatchNormalization())\n",
        "   model.add(layers.LeakyReLU())\n",
        "\n",
        "   model.add(layers.Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', use_bias=False )) \n",
        "   assert model.output_shape == (None, 256, 256, 128)\n",
        "   model.add(layers.BatchNormalization())\n",
        "   model.add(layers.LeakyReLU())\n",
        "\n",
        "   model.add(layers.Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', use_bias=False )) \n",
        "   assert model.output_shape == (None,512,512,64)\n",
        "   model.add(layers.BatchNormalization())\n",
        "   model.add(layers.LeakyReLU())\n",
        "\n",
        "   model.add(layers.Conv2DTranspose(3,(3,3),strides=(1,1), padding='same', use_bias=False ))\n",
        "   assert model.output_shape == (None, 512, 512, 3)\n",
        "\n",
        "   return model\n",
        "\n",
        "# Gen = make_generator_model()   \n"
      ],
      "metadata": {
        "id": "3vyG_cwHhx44"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DISCREMENATOR\n",
        "def make_discremenator_model():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Conv2D(64,(3,3),strides=(2,2),padding='same',input_shape=[512,512,3]))\n",
        "  model.add(layers.LeakyReLU)\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Conv2D(128,(3,3),strides=(2,2),padding='same'))\n",
        "  model.add(layers.LeakyReLU)\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Conv2D(256,(3,3),strides=(2,2),padding='same'))\n",
        "  model.add(layers.LeakyReLU)\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Conv2D(512,(3,3),strides=(2,2),padding='same'))\n",
        "  model.add(layers.LeakyReLU)\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64))\n",
        "  model.add(layers.Dense(1))\n",
        "  \n",
        "  return model\n",
        "\n",
        "# decrm = make_discremenator_model()  "
      ],
      "metadata": {
        "id": "MQQH1-xUhBAR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_math_ops import cross\n",
        "# define loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# discremenator loss\n",
        "def discremenator_loss(real_output,fake_output):\n",
        "  real_loss =cross_entropy(tf.ones_like(real_output),real_output)\n",
        "  fake_loss =cross_entropy(tf.zeros_like(fake_output),fake_output)\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "# generator loss\n",
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output),fake_output)"
      ],
      "metadata": {
        "id": "ohSuGKySjPYg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimizers\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discremenator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "metadata": {
        "id": "u_srwanpm4Af"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save checkpoints\n",
        "\n",
        "checkpoint_dir = 'gdrive/mydrive/pic'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discremenator_optimizer=discremenator_optimizer, generator=gen, discremenator=dscrm)"
      ],
      "metadata": {
        "id": "NbV2p7uUo2Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "EPOCHS = 500\n",
        "noise_dim = 100\n",
        "num_example_to_generate = 4\n",
        "\n",
        "# seed to reuse\n",
        "seed = tf.random.normal([num_example_to_generate,noise_dim])"
      ],
      "metadata": {
        "id": "2JT2KCZppZXj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  noise = tf.random.normal([BATCH_SIZE,noise_dim])\n",
        "\n",
        "  with tf.GradientTape() as gen_tap, tf.GradientTape() as Disc_tap:\n",
        "    generated_images = gen(noise,training=True)\n",
        "    real_output = descrm(images,training=True)\n",
        "    fake_output = descrm(generated_images,training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    descrm_loss = discremenator_loss(real_output,fake_output)\n",
        "\n",
        "    gradiants_of_generator = gen_tap.gradient(gen_loss, gen.traniable_variables)\n",
        "    gradiants_of_discremenator = disc_tap.gradient(disc_loss,dscrm.traniable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradiants_of_generator, gen.traniable_variables))\n",
        "    discremenator_optimizer.apply_gradients(zip(gradiants_of_discremenator, dscrm.traniable_variables))\n"
      ],
      "metadata": {
        "id": "0RlsFISFrnMW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_image(model,epochs,test_intput):\n",
        "  predictions = model(test_intput,training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    # denormalize_images\n",
        "    plt.imshow(np.array((predictions[i, :, :, :]*127.5 + 127.5), np.int32))\n",
        "    plt.axis('off')\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show\n"
      ],
      "metadata": {
        "id": "OhC719oyyECS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset ,epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_patch in dataset:\n",
        "      train_step(image_patch)\n",
        "\n",
        "    # produce image\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_image(gen,epoch +1,seed)\n",
        "\n",
        "    # save model\n",
        "    if (epoch +1)%5 == 0 :\n",
        "      gen.save(checkpoint_dir)\n",
        "    print('time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))  \n",
        "  display.clear_output(wait=True)  \n",
        "  generate_and_save_image(gen, epochs, seed)"
      ],
      "metadata": {
        "id": "tve8CCOuzD7g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "id": "yhJmXN3t2GI7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}